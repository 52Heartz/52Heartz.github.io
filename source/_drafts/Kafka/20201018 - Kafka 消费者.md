---
title: Kafka 消费者
mathjax: true
date: 2020-10-18 11:51:58
updated:
categories:
tags:
urlname: about-kafka-consumers
---



<!-- more -->

Kafka Documentation：

Consumer 官方 Javadoc：[KafkaConsumer (kafka 2.2.0 API)](https://kafka.apache.org/22/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html)



# 基本概念

## Consumer Group

消费同一个 topic 的时候，给多个消费者设定相同的 `consumer.group` 属性，可以使多个 consumer 构成一个 consumer group，这一个组内的消费者分别消费这个 topic 的不同 partition。



> Keep in mind that there is no point in adding more consumers than you have partitions in a topic—some of the consumers will just be idle.



# 创建 Kafka 消费者

三个必须的属性：

1. bootstrap.servers
2. key.deserializer
3. value.deserializer



```java
Properties props = new Properties();
props.put("bootstrap.servers", "broker1:9092,broker2:9092");
props.put("group.id", "CountryCounter");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
KafkaConsumer<String, String> consumer = new KafkaConsumer<String, String>(props);
consumer.subscribe(Collections.singletonList(topicName))
```



## 取数据

```java
try {
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(100);
        for (ConsumerRecord<String, String> record : records)
        {
            log.debug("topic = %s, partition = %d, offset = %d,"
                customer = %s, country = %s\n",
                record.topic(), record.partition(), record.offset(),
                record.key(), record.value());

            int updatedCount = 1;
            if (custCountryMap.countainsKey(record.value())) {
                updatedCount = custCountryMap.get(record.value()) + 1;
            }
            custCountryMap.put(record.value(), updatedCount)

            JSONObject json = new JSONObject(custCountryMap);
            System.out.println(json.toString(4))
        }
    }
} finally {
    consumer.close();
}
```



`consumer.poll(100)` 其中的 100 就是从 Kafka 取数据的时候等待的时间。如果设置为 0，那么调用 `poll()` 后会立刻返回。

> If this is set to 0, `poll()` will return immediately; otherwise, it will wait for the specified number of milliseconds for data to arrive from the broker.



## 关于线程安全

consumer 不能多线程共用，只能一个线程使用一个 consumer。



## 关键配置项

### fetch.min.bytes

最小拉取的消息的大小。consumer 发起拉取请求的时候，如果 Kafka 中没有足够数量的消息，那么 Kafka 会等待直到消息数量满足这个标准才会返回数据给 consumer。

Q：Consumer 会等待还是取到空数据？



### fetch.max.wait.ms

拉取时的最大等待时间。



### session.timeout.ms

> The amount of time a consumer can be out of contact with the brokers while still considered alive defaults to 10 seconds. If more than `session.timeout.ms` passes without the consumer sending a heartbeat to the group coordinator, it is considered dead and the group coordinator will trigger a rebalance of the consumer group to allocate partitions from the dead consumer to the other consumers in the group.

如果一个消费者两次发送心跳包的时间间隔超过了这个配置项中规定的时间，那么 Kafka 会认为这个消费者已经死了，会触发一次 consumer rebalence。



### auto.offset.reset

有效值：

1. latest
2. earliest
3. none

> This property controls the behavior of the consumer when it starts reading a partition for which it doesn’t have a committed offset or if the committed offset it has is invalid (usually because the consumer was down for so long that the record with that offset was already aged out of the broker).

默认值是 `latest`，表示如果当前消费者如果在 Kafka 中没有记录的 offset 的话，会从最新的数据开始读取，即从当前消费者启动之后对应 topic 中产生的新数据开始消费。

设置为 none 的话，如果从一个无效的 offset 开始消费，会抛出异常。

> Setting `auto.offset.reset` to `none` will cause an exception to be thrown when attempting to consume from invalid offset.



### enable.auto.commit



### max.poll.records





### max.partition.fetch.bytes

> The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress. The maximum record batch size accepted by the broker is defined via `message.max.bytes` (broker config) or `max.message.bytes` (topic config). See fetch.max.bytes for limiting the consumer request size.



### fetch.max.bytes

> The maximum amount of data the server should return for a fetch request. Records are fetched in batches by the consumer, and if the first record batch in the first non-empty partition of the fetch is larger than this value, the record batch will still be returned to ensure that the consumer can make progress. As such, this is not a absolute maximum. The maximum record batch size accepted by the broker is defined via `message.max.bytes` (broker config) or `max.message.bytes` (topic config). Note that the consumer performs multiple fetches in parallel.



# 拉取消息调优

[Increase the number of messages read by a Kafka consumer in a single poll - Stack Overflow](https://stackoverflow.com/questions/51753883/increase-the-number-of-messages-read-by-a-kafka-consumer-in-a-single-poll/51755259)







# 手动 commit

## commitSync



## commitAsync



# Consumer Rebalence



> If you are using a new version and need to handle records that take longer to process, you simply need to tune `max.poll.interval.ms` so it will handle longer delays between polling for new records.



# Subscribe Vs Assign

给 Consumer 分配 topicPartition 的时候有两种方式，一种是 Consumer#subscribe，另一种是 Consumer#assign。

简而言之，assign 不会用到 Kafka 的 group management 机制，即使 consumer 变化，也不会触发 rebalance。



[kafka consumer assign 和 subscribe模式差异分析 - sanmutongzi - 博客园](https://www.cnblogs.com/dongxiao-yang/p/7200971.html)

[apache kafka - KafkaConsumer Java API subscribe() vs assign() - Stack Overflow](https://stackoverflow.com/questions/53938125/kafkaconsumer-java-api-subscribe-vs-assign/53938397)



# 常见场景分析

## 一个 Consumer 订阅多个 topic

潜在问题：

1. 假设订阅了两个 topic，分别为 t1 和 t2，t1 中的消息增加的非常快，每秒增加 500 条，t2 中每秒增加 5 条。消费的时候每秒拉取 100 条，那么消费端可能会一直只拉取到 t1 中的消息，拉取不到 t2 中的消息。



> I recently had a bug because my application subscribed to many topics with a single Consumer. Each topic was a live feed of images at one image per message. Since all the topics always had new images, each poll() was only returning images from the first topic to register.
>
> If processing all messages is important, you'll need to be certain that each Consumer can process messages from all of its subscribed topics faster than the messages are created. If it can't, you'll either need more Consumers committing reads in the same group, or you'll have to be OK with the fact that some messages may never be processed.
>
> [multithreading - what is best practice to consume messages from multiple kafka topics? - Stack Overflow](https://stackoverflow.com/questions/46628946/what-is-best-practice-to-consume-messages-from-multiple-kafka-topics)



## 获取 Consumer 当前的 Offset



使用 `KafkaConsumer#committed()`

> You can use `KafkaConsumer#committed()` to get the latest committed position.
>
> ——[How to get current offset in Kafka 0.10.x without shell? - Stack Overflow](https://stackoverflow.com/questions/41692805/how-to-get-current-offset-in-kafka-0-10-x-without-shell)





# 一些关于 Kafka 消费者理解的澄清

之前一直以为 kafka 生产者和消费者都是一个独立的线程，实际上消费者是没有独立的线程的，commitSync 和 commitAsync 都依赖于不断地 poll()，比如 commitAsync 就会在 poll 的时候触发。







# 参考资料

1. [4. Kafka Consumers: Reading Data from Kafka - Kafka: The Definitive Guide](https://www.oreilly.com/library/view/kafka-the-definitive/9781491936153/ch04.html#callout_kafka_consumers__reading_data_from_kafka_CO12-4)
2. [Multi-Threaded Messaging with the Apache Kafka Consumer](https://www.confluent.io/blog/kafka-consumer-multi-threaded-messaging/)
3. [A Dive into Multi-Topic Subscriptions with Apache Kafka - CloudKarafka, Apache Kafka Message streaming as a Service](https://www.cloudkarafka.com/blog/2019-09-11-a-dive-into-multi-topic-subscriptions-with-apache-kafka.html)