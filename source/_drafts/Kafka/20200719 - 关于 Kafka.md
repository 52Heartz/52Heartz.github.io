---
title: 关于 Kafka
mathjax: true
date: 2020-07-19 21:37:01
updated:
categories:
tags:
urlname: about-kafka
---



<!-- more -->

官网：[Apache Kafka](https://kafka.apache.org/)



官网的介绍：[Introduction - Apache Kafka](https://kafka.apache.org/intro)

Javadoc：[Admin (kafka 2.6.0 API)](http://kafka.apache.org/26/javadoc/index.html?org/apache/kafka/clients/admin/Admin.html)



# Topic 设置

[Apache Kafka topic config](http://kafka.apache.org/documentation.html#topicconfigs)



## retention.bytes

> This configuration controls the maximum size a partition (which consists of log segments) can grow to before we will discard old log segments to free up space if we are using the "delete" retention policy. By default there is no size limit only a time limit. Since this limit is enforced at the partition level, multiply it by the number of partitions to compute the topic retention in bytes.



## cleanup.policy

> Log compaction ensures that Kafka will always retain at least the last known value for each message key within the log of data for a single topic partition. It addresses use cases and scenarios such as restoring state after application crashes or system failure, or reloading caches after application restarts during operational maintenance. Let's dive into these use cases in more detail and then describe how compaction works.

Log compaction 可以确保，在一个 partition 中，Kafka 会保证 key 相同的消息，至少保留最后的一条数据。





[4.8 Log Compaction - Apache Kafka](https://kafka.apache.org/documentation/#compaction)

[The Log: What every software engineer should know about real-time data's unifying abstraction | LinkedIn Engineering](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)



[kafka日志清理策略，compact和delete_寒夜-CSDN博客](https://blog.csdn.net/u013200380/article/details/106453013)

[Apache Kafka（十二）Log Cleanup 策略 - ZacksTang - 博客园](https://www.cnblogs.com/zackstang/p/11638127.html)

[【Kafka 简明原理】你从来没听说过的日志紧缩](https://juejin.cn/post/6844904003868835853)







# 关于 Kafka Producer

[kafka producer的batch.size和linger.ms](https://www.cnblogs.com/set-cookie/p/8902340.html)

[Producer Configurations — Confluent Platform 5.5.1](https://docs.confluent.io/current/installation/configuration/producer-configs.html)



## 高可用（无消息丢失）

[Kafka无消息丢失配置 - huxihx - 博客园](https://www.cnblogs.com/huxi2b/p/6056364.html)



# Kafka Message Overhead

[java - What is size overhead for a message in Kafka? - Stack Overflow](https://stackoverflow.com/questions/57472830/what-is-size-overhead-for-a-message-in-kafka)

[java - Understanding Kafka Message Byte Size - Stack Overflow](https://stackoverflow.com/questions/56675681/understanding-kafka-message-byte-size)



# 其他

一个开源的 Kafka 管理平台：[xaecbd/KafkaCenter: KafkaCenter is a unified one-stop platform for Kafka cluster management and maintenance, producer / consumer monitoring, and use of ecological components.](https://github.com/xaecbd/KafkaCenter)





# Kafka JMX Metrics

https://kafka.apache.org/documentation/#monitoring

https://kafka.apache.org/documentation/#selector_monitoring





# 性能调优

[记一次 Kafka Producer 性能调优实战](http://objcoding.com/2020/09/18/kafka-producer-performance-optimization/)

[Kafka Producer配置解读 // 乱世浮生](https://atbug.com/kafka-producer-config/#lingerms)

[kafka消息可靠性 | LP's Notes](http://www.lpnote.com/2017/01/15/reliability-of-kafka-message/)

[Kafka从上手到实践-庖丁解牛：Producer | 程序员说](https://www.devtalking.com/articles/kafka-practice-4/)

[Kafka 源码解析之 Producer 单 Partition 顺序性实现及配置说明（五） | Matt's Blog](https://matt33.com/2017/09/10/produccer-end/)

[Kafka无消息丢失配置 - huxihx - 博客园](https://www.cnblogs.com/huxi2b/p/6056364.html)







# 常见需求

## 查看 Topic 大小

[See size of Kafka Topics in Bytes - Stack Overflow](https://stackoverflow.com/questions/43473670/see-size-of-kafka-topics-in-bytes)

> You can see the partition size using the script /bin/kafka-log-dirs.sh
>
> ```
> /bin/kafka-log-dirs.sh --describe --bootstrap-server <KafakBrokerHost>:<KafakBrokerPort> --
> ```

上边的回答给出了使用脚本获取文件夹大小，从而得知 topic 大小的方式。



如果是用 Kafka Client，可以参考下边的方法：



```java
public static TopicDiskSizeOffsetDescriber getTopicDiskSizeForBroker(String kafkaServers, int brokerID)
    throws Exception {
    AdminClient adminClient = null;
    TopicDiskSizeOffsetDescriber topicDiskSizeOffsetDescriber = new TopicDiskSizeOffsetDescriber();
    try {
        adminClient = getAdminClient(kafkaServers);
        DescribeLogDirsResult logDirsResult = adminClient.describeLogDirs(Collections.singletonList(brokerID));
        Map<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> tmp = logDirsResult.all().get(10, TimeUnit.SECONDS);
        for (Map.Entry<Integer, Map<String, DescribeLogDirsResponse.LogDirInfo>> entry : tmp.entrySet()) {
            Map<String, DescribeLogDirsResponse.LogDirInfo> tmp1 = entry.getValue();
            for (Map.Entry<String, DescribeLogDirsResponse.LogDirInfo> entry1 : tmp1.entrySet()) {
                DescribeLogDirsResponse.LogDirInfo info = entry1.getValue();
                Map<TopicPartition, DescribeLogDirsResponse.ReplicaInfo> replicaInfoMap = info.replicaInfos;
                replicaInfoMap.entrySet().stream().collect(Collectors.groupingBy(e -> e.getKey().topic(), LinkedHashMap::new, Collectors.toList()))
                    .entrySet().forEach(en -> {
                    String topic = en.getKey();
                    if(StringUtils.equals(topic, "__consumer_offsets")) {
                        return;
                    }
                    TopicDiskSizeOffsetDescriber.TopicPartitionSizeOffset topicSizeOffset = new TopicDiskSizeOffsetDescriber.TopicPartitionSizeOffset(topic);
                    en.getValue().forEach(replicas -> {
                        topicSizeOffset.addPartitionSizeOffset(replicas.getKey().partition(), replicas.getValue().size, replicas.getValue().offsetLag);
                    });
                    topicDiskSizeOffsetDescriber.addTopicPartitionSizeOffset(topicSizeOffset);
                });
            }
        }
    } finally {
        if(adminClient != null)
            adminClient.close();
    }
    return topicDiskSizeOffsetDescriber;
}
```







# 删除 Topic



通过 Shell 删除：

```sh
./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic 'finetube_v2_pipeline_78062a1b-0b15-4e3a-ae58-d113bc82f9da.*'
```

结果是：

```
Topic finetube_v2_pipeline_78062a1b-0b15-4e3a-ae58-d113bc82f9da_EAS1104.CT_BAS_SALEAREA is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
```



刚刚删除完之后，文件夹不会清除，而是会改为后边加上 `...delete` 后缀。稍等一会儿之后，才会真正删除。

```
[root@bi-finetube kafka-logs]# du -sh * | sort -rh | head -1
16G	finetube_v2_pipeline_78062a1b-0b15-4e3a-ae58-d113bc82f9da_EAS1104.T_IM_SALEISSUEENTRY-0
[root@bi-finetube kafka-logs]# du -sh * | sort -rh | head -1
16G	finetube_v2_pipeline_78062a1b-0b15-4e3a-ae58-d113bc82f9da_EAS1104.T_IM_SALEISSUEENTRY-0.7c3a74f544d7445493017bbd1b8dbd5f-delete
```





# 维护

## 查看部署的 Kafka 版本

主要是通过看 Kafka 的 JAR 版本来确认。

```
find ./libs/ -name \*kafka_\* | head -1 | grep -o '\kafka[^\n]*'
```





# 参考资料

1. [Apache Kafka](https://kafka.apache.org/)
2. [再过半小时，你就能明白kafka的工作原理了 - 苏苏喂苏苏+ - 博客园](https://www.cnblogs.com/sujing/p/10960832.html)
3. [震惊了！原来这才是 Kafka！（多图+深入） - 知乎](https://zhuanlan.zhihu.com/p/158936520)
4. [消息队列的使用场景是怎样的？ - 知乎](https://www.zhihu.com/question/34243607)
5. [Manually delete Apache Kafka topics | by Sunny Srinidhi | Medium](https://contactsunny.medium.com/manually-delete-apache-kafka-topics-424c7e016ff3)